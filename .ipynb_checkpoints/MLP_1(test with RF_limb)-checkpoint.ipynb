{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a267ce36-2ffd-4130-bce5-ae2629917110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f857b-6360-4d0c-ba00-270c0c3041da",
   "metadata": {},
   "source": [
    "### The MLP to predict the force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85911096-4736-41e6-bc9a-7a5f7e8b9bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=70, hidden_size=32, output_size=70):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60be34f3-b640-48d6-8874-80d6f864472e",
   "metadata": {},
   "source": [
    "### Prepare the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e780585-cd52-4f01-9f71-d86b39670e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_samples(data, sample_length=70):\n",
    "    n = len(data)\n",
    "    num_samples = n // sample_length  # Number of complete samples\n",
    "    trimmed_data = data[:num_samples * sample_length]\n",
    "    reshaped = trimmed_data.reshape((num_samples, sample_length))\n",
    "    return reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc579acc-ac44-4542-aba4-a6d73a38a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 140\n",
    "data_in = np.load('data_in_for_predict.npy')\n",
    "data_out = np.load('data_out_for_predict.npy')\n",
    "\n",
    "# select y to train as an example\n",
    "X_scaled = reshape_to_samples(data_in[:,1])\n",
    "Y_scaled = reshape_to_samples(data_out[:,1])\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y_scaled, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd55b464-a5f2-466e-8e7f-7bee851e5bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split into training+validation and test\n",
    "X_temp, X_test, Y_temp, Y_test = train_test_split(\n",
    "    X_tensor, Y_tensor, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Then split training+validation into training and validation\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_temp, Y_temp, test_size=0.1765, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8efca0f-868c-44ee-a318-bef423c5f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "val_dataset   = TensorDataset(X_val, Y_val)\n",
    "test_dataset  = TensorDataset(X_test, Y_test)  # NEW\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16)  # NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6685e391-7f45-4a07-ab54-8435345fc72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ======= Use GPU if available =======\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f189669a-3ee9-4244-9b57-bf9a19accc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Initialize model, loss, optimizer =======\n",
    "model = MLP().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5df8fa27-3b7a-44eb-af60-d16c2389a8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Validation Loss: 1.085925\n",
      "Epoch 2/1000, Validation Loss: 1.003548\n",
      "Epoch 3/1000, Validation Loss: 0.884191\n",
      "Epoch 4/1000, Validation Loss: 0.708752\n",
      "Epoch 5/1000, Validation Loss: 0.474132\n",
      "Epoch 6/1000, Validation Loss: 0.235016\n",
      "Epoch 7/1000, Validation Loss: 0.115719\n",
      "Epoch 8/1000, Validation Loss: 0.098342\n",
      "Epoch 9/1000, Validation Loss: 0.064313\n",
      "Epoch 10/1000, Validation Loss: 0.052248\n",
      "Epoch 11/1000, Validation Loss: 0.051341\n",
      "Epoch 12/1000, Validation Loss: 0.049831\n",
      "Epoch 13/1000, Validation Loss: 0.050112\n",
      "Epoch 14/1000, Validation Loss: 0.049248\n",
      "Epoch 15/1000, Validation Loss: 0.048342\n",
      "Epoch 16/1000, Validation Loss: 0.048080\n",
      "Epoch 17/1000, Validation Loss: 0.047879\n",
      "Epoch 18/1000, Validation Loss: 0.047594\n",
      "Epoch 19/1000, Validation Loss: 0.046661\n",
      "Epoch 20/1000, Validation Loss: 0.046474\n",
      "Epoch 21/1000, Validation Loss: 0.046069\n",
      "Epoch 22/1000, Validation Loss: 0.045791\n",
      "Epoch 23/1000, Validation Loss: 0.044792\n",
      "Epoch 24/1000, Validation Loss: 0.044436\n",
      "Epoch 25/1000, Validation Loss: 0.044263\n",
      "Epoch 26/1000, Validation Loss: 0.043809\n",
      "Epoch 27/1000, Validation Loss: 0.042811\n",
      "Epoch 28/1000, Validation Loss: 0.042073\n",
      "Epoch 29/1000, Validation Loss: 0.041683\n",
      "Epoch 30/1000, Validation Loss: 0.041130\n",
      "Epoch 31/1000, Validation Loss: 0.040825\n",
      "Epoch 32/1000, Validation Loss: 0.040262\n",
      "Epoch 33/1000, Validation Loss: 0.039842\n",
      "Epoch 34/1000, Validation Loss: 0.038836\n",
      "Epoch 35/1000, Validation Loss: 0.038446\n",
      "Epoch 36/1000, Validation Loss: 0.038122\n",
      "Epoch 37/1000, Validation Loss: 0.037820\n",
      "Epoch 38/1000, Validation Loss: 0.037602\n",
      "Epoch 39/1000, Validation Loss: 0.036789\n",
      "Epoch 40/1000, Validation Loss: 0.036815\n",
      "Epoch 41/1000, Validation Loss: 0.036361\n",
      "Epoch 42/1000, Validation Loss: 0.035998\n",
      "Epoch 43/1000, Validation Loss: 0.035347\n",
      "Epoch 44/1000, Validation Loss: 0.035267\n",
      "Epoch 45/1000, Validation Loss: 0.034884\n",
      "Epoch 46/1000, Validation Loss: 0.034596\n",
      "Epoch 47/1000, Validation Loss: 0.034210\n",
      "Epoch 48/1000, Validation Loss: 0.033889\n",
      "Epoch 49/1000, Validation Loss: 0.033798\n",
      "Epoch 50/1000, Validation Loss: 0.033597\n",
      "Epoch 51/1000, Validation Loss: 0.033034\n",
      "Epoch 52/1000, Validation Loss: 0.032727\n",
      "Epoch 53/1000, Validation Loss: 0.032554\n",
      "Epoch 54/1000, Validation Loss: 0.032406\n",
      "Epoch 55/1000, Validation Loss: 0.031999\n",
      "Epoch 56/1000, Validation Loss: 0.031554\n",
      "Epoch 57/1000, Validation Loss: 0.030962\n",
      "Epoch 58/1000, Validation Loss: 0.030576\n",
      "Epoch 59/1000, Validation Loss: 0.030745\n",
      "Epoch 60/1000, Validation Loss: 0.030595\n",
      "Epoch 61/1000, Validation Loss: 0.030180\n",
      "Epoch 62/1000, Validation Loss: 0.030002\n",
      "Epoch 63/1000, Validation Loss: 0.029555\n",
      "Epoch 64/1000, Validation Loss: 0.029293\n",
      "Epoch 65/1000, Validation Loss: 0.028949\n",
      "Epoch 66/1000, Validation Loss: 0.028610\n",
      "Epoch 67/1000, Validation Loss: 0.028384\n",
      "Epoch 68/1000, Validation Loss: 0.027969\n",
      "Epoch 69/1000, Validation Loss: 0.027633\n",
      "Epoch 70/1000, Validation Loss: 0.027652\n",
      "Epoch 71/1000, Validation Loss: 0.027145\n",
      "Epoch 72/1000, Validation Loss: 0.026977\n",
      "Epoch 73/1000, Validation Loss: 0.026544\n",
      "Epoch 74/1000, Validation Loss: 0.026542\n",
      "Epoch 75/1000, Validation Loss: 0.026407\n",
      "Epoch 76/1000, Validation Loss: 0.026797\n",
      "Epoch 77/1000, Validation Loss: 0.026031\n",
      "Epoch 78/1000, Validation Loss: 0.025429\n",
      "Epoch 79/1000, Validation Loss: 0.025481\n",
      "Epoch 80/1000, Validation Loss: 0.025107\n",
      "Epoch 81/1000, Validation Loss: 0.025250\n",
      "Epoch 82/1000, Validation Loss: 0.024920\n",
      "Epoch 83/1000, Validation Loss: 0.024560\n",
      "Epoch 84/1000, Validation Loss: 0.024353\n",
      "Epoch 85/1000, Validation Loss: 0.024407\n",
      "Epoch 86/1000, Validation Loss: 0.024390\n",
      "Epoch 87/1000, Validation Loss: 0.024072\n",
      "Epoch 88/1000, Validation Loss: 0.023772\n",
      "Epoch 89/1000, Validation Loss: 0.023483\n",
      "Epoch 90/1000, Validation Loss: 0.023234\n",
      "Epoch 91/1000, Validation Loss: 0.023463\n",
      "Epoch 92/1000, Validation Loss: 0.022947\n",
      "Epoch 93/1000, Validation Loss: 0.022799\n",
      "Epoch 94/1000, Validation Loss: 0.022398\n",
      "Epoch 95/1000, Validation Loss: 0.022806\n",
      "Epoch 96/1000, Validation Loss: 0.022684\n",
      "Epoch 97/1000, Validation Loss: 0.021940\n",
      "Epoch 98/1000, Validation Loss: 0.021946\n",
      "Epoch 99/1000, Validation Loss: 0.021890\n",
      "Epoch 100/1000, Validation Loss: 0.021691\n",
      "Epoch 101/1000, Validation Loss: 0.022158\n",
      "Epoch 102/1000, Validation Loss: 0.021312\n",
      "Epoch 103/1000, Validation Loss: 0.021505\n",
      "Epoch 104/1000, Validation Loss: 0.021595\n",
      "Epoch 105/1000, Validation Loss: 0.021129\n",
      "Epoch 106/1000, Validation Loss: 0.020864\n",
      "Epoch 107/1000, Validation Loss: 0.020655\n",
      "Epoch 108/1000, Validation Loss: 0.020496\n",
      "Epoch 109/1000, Validation Loss: 0.020431\n",
      "Epoch 110/1000, Validation Loss: 0.020668\n",
      "Epoch 111/1000, Validation Loss: 0.020182\n",
      "Epoch 112/1000, Validation Loss: 0.020669\n",
      "Epoch 113/1000, Validation Loss: 0.020016\n",
      "Epoch 114/1000, Validation Loss: 0.020404\n",
      "Epoch 115/1000, Validation Loss: 0.020000\n",
      "Epoch 116/1000, Validation Loss: 0.019646\n",
      "Epoch 117/1000, Validation Loss: 0.019741\n",
      "Epoch 118/1000, Validation Loss: 0.019738\n",
      "Epoch 119/1000, Validation Loss: 0.019401\n",
      "Epoch 120/1000, Validation Loss: 0.019268\n",
      "Epoch 121/1000, Validation Loss: 0.019616\n",
      "Epoch 122/1000, Validation Loss: 0.019650\n",
      "Epoch 123/1000, Validation Loss: 0.019468\n",
      "Epoch 124/1000, Validation Loss: 0.019213\n",
      "Epoch 125/1000, Validation Loss: 0.019072\n",
      "Epoch 126/1000, Validation Loss: 0.019264\n",
      "Epoch 127/1000, Validation Loss: 0.018994\n",
      "Epoch 128/1000, Validation Loss: 0.018766\n",
      "Epoch 129/1000, Validation Loss: 0.019244\n",
      "Epoch 130/1000, Validation Loss: 0.018553\n",
      "Epoch 131/1000, Validation Loss: 0.018901\n",
      "Epoch 132/1000, Validation Loss: 0.018355\n",
      "Epoch 133/1000, Validation Loss: 0.018885\n",
      "Epoch 134/1000, Validation Loss: 0.018724\n",
      "Epoch 135/1000, Validation Loss: 0.018506\n",
      "Epoch 136/1000, Validation Loss: 0.018824\n",
      "Epoch 137/1000, Validation Loss: 0.018371\n",
      "Epoch 138/1000, Validation Loss: 0.018693\n",
      "Epoch 139/1000, Validation Loss: 0.018407\n",
      "Epoch 140/1000, Validation Loss: 0.018568\n",
      "Epoch 141/1000, Validation Loss: 0.018000\n",
      "Epoch 142/1000, Validation Loss: 0.018792\n",
      "Epoch 143/1000, Validation Loss: 0.017938\n",
      "Epoch 144/1000, Validation Loss: 0.018509\n",
      "Epoch 145/1000, Validation Loss: 0.018020\n",
      "Epoch 146/1000, Validation Loss: 0.018583\n",
      "Epoch 147/1000, Validation Loss: 0.018106\n",
      "Epoch 148/1000, Validation Loss: 0.018002\n",
      "Epoch 149/1000, Validation Loss: 0.018070\n",
      "Epoch 150/1000, Validation Loss: 0.017961\n",
      "Epoch 151/1000, Validation Loss: 0.017955\n",
      "Epoch 152/1000, Validation Loss: 0.017877\n",
      "Epoch 153/1000, Validation Loss: 0.017920\n",
      "Epoch 154/1000, Validation Loss: 0.017819\n",
      "Epoch 155/1000, Validation Loss: 0.017759\n",
      "Epoch 156/1000, Validation Loss: 0.017667\n",
      "Epoch 157/1000, Validation Loss: 0.017879\n",
      "Epoch 158/1000, Validation Loss: 0.017648\n",
      "Epoch 159/1000, Validation Loss: 0.018039\n",
      "Epoch 160/1000, Validation Loss: 0.017458\n",
      "Epoch 161/1000, Validation Loss: 0.017886\n",
      "Epoch 162/1000, Validation Loss: 0.017912\n",
      "Epoch 163/1000, Validation Loss: 0.017713\n",
      "Epoch 164/1000, Validation Loss: 0.017538\n",
      "Epoch 165/1000, Validation Loss: 0.017453\n",
      "Epoch 166/1000, Validation Loss: 0.017468\n",
      "Epoch 167/1000, Validation Loss: 0.017479\n",
      "Epoch 168/1000, Validation Loss: 0.017380\n",
      "Epoch 169/1000, Validation Loss: 0.017307\n",
      "Epoch 170/1000, Validation Loss: 0.017511\n",
      "Epoch 171/1000, Validation Loss: 0.017405\n",
      "Epoch 172/1000, Validation Loss: 0.017412\n",
      "Epoch 173/1000, Validation Loss: 0.017129\n",
      "Epoch 174/1000, Validation Loss: 0.017464\n",
      "Epoch 175/1000, Validation Loss: 0.017496\n",
      "Epoch 176/1000, Validation Loss: 0.017199\n",
      "Epoch 177/1000, Validation Loss: 0.017296\n",
      "Epoch 178/1000, Validation Loss: 0.017323\n",
      "Epoch 179/1000, Validation Loss: 0.017088\n",
      "Epoch 180/1000, Validation Loss: 0.017401\n",
      "Epoch 181/1000, Validation Loss: 0.016995\n",
      "Epoch 182/1000, Validation Loss: 0.017191\n",
      "Epoch 183/1000, Validation Loss: 0.017552\n",
      "Epoch 184/1000, Validation Loss: 0.016995\n",
      "Epoch 185/1000, Validation Loss: 0.017434\n",
      "Epoch 186/1000, Validation Loss: 0.017123\n",
      "Epoch 187/1000, Validation Loss: 0.017321\n",
      "Epoch 188/1000, Validation Loss: 0.017418\n",
      "Epoch 189/1000, Validation Loss: 0.017130\n",
      "Epoch 190/1000, Validation Loss: 0.017070\n",
      "Epoch 191/1000, Validation Loss: 0.017174\n",
      "Epoch 192/1000, Validation Loss: 0.016996\n",
      "Epoch 193/1000, Validation Loss: 0.017087\n",
      "Epoch 194/1000, Validation Loss: 0.017115\n",
      "Epoch 195/1000, Validation Loss: 0.017027\n",
      "Epoch 196/1000, Validation Loss: 0.017497\n",
      "Epoch 197/1000, Validation Loss: 0.017055\n",
      "Epoch 198/1000, Validation Loss: 0.017172\n",
      "Epoch 199/1000, Validation Loss: 0.017059\n",
      "Epoch 200/1000, Validation Loss: 0.017032\n",
      "Epoch 201/1000, Validation Loss: 0.017110\n",
      "Early stopping at epoch 201, best val loss: 0.016995\n",
      "Model on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ======= Training loop with Early Stopping =======\n",
    "num_epochs = 1000\n",
    "best_val_loss = float('inf')\n",
    "patience = 20        # stop if no improvement for 20 epochs\n",
    "wait = 0             # epochs since last improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        Y_batch = Y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # ======= Validation =======\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for X_batch, Y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            Y_batch = Y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            val_loss += criterion(outputs, Y_batch).item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "    # ======= Check Early Stopping Condition =======\n",
    "    if val_loss < best_val_loss - 1e-5:  # small threshold to detect actual improvement\n",
    "        best_val_loss = val_loss\n",
    "        wait = 0\n",
    "        best_model_state = model.state_dict()  # save best model\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}, best val loss: {best_val_loss:.6f}\")\n",
    "            break\n",
    "\n",
    "# ======= Load best model =======\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# ======= Confirm device =======\n",
    "print(\"Model on:\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0a77705-a16a-4a62-9b81-a0a5b9b3b4f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaler_Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     true_vals = Y_val.cpu().numpy()\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# ======= Inverse scaling (optional, if you scaled your outputs) =======\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m predictions_original = \u001b[43mscaler_Y\u001b[49m.inverse_transform(predictions)\n\u001b[32m     10\u001b[39m true_vals_original = scaler_Y.inverse_transform(true_vals)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# ======= Plotting for first few samples =======\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'scaler_Y' is not defined"
     ]
    }
   ],
   "source": [
    "# ======= Evaluation (Predictions) =======\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_val_gpu = X_val.to(device)\n",
    "    predictions = model(X_val_gpu).cpu().numpy()  # move back to CPU for plotting\n",
    "    true_vals = Y_val.cpu().numpy()\n",
    "\n",
    "# ======= Inverse scaling (optional, if you scaled your outputs) =======\n",
    "predictions_original = scaler_Y.inverse_transform(predictions)\n",
    "true_vals_original = scaler_Y.inverse_transform(true_vals)\n",
    "\n",
    "# ======= Plotting for first few samples =======\n",
    "n_samples_to_plot = 5\n",
    "for i in range(n_samples_to_plot):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(true_vals_original[i], label=\"True\", linewidth=2)\n",
    "    plt.plot(predictions_original[i], label=\"Predicted\", linestyle='--')\n",
    "    plt.title(f\"Sample {i + 1}\")\n",
    "    plt.xlabel(\"Feature Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b7e02-c787-4670-9987-3caa880b96fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nyxbot)",
   "language": "python",
   "name": "nyxbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
